#### 활성화 함수
입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수라 합니다. `활성화`라는 이름이 말해주듯 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 합니다.

```js
const h = x => x > 0 ? 1 : 0;
const a = b + w1 * x1 + w2 * x2;
const y = h(a);
```

### 계단 함수
퍼셉트론은 활성화 함수 중 계단 함수를 사용하고 있습니다. 임계값을 경계로 출력이 바뀌는 데, 이런 함수를 계단 함수라 합니다.

#### 시그모이드 함수
신경망에서 자주 이용하는 활성화 함수인 시그모이드 함수를 나타낸 식입니다. 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달합니다. 앞 장에서 본 퍼셉트론과 앞으로 볼 신경망의 주된 차이는 이 활성화 함수뿐입니다.
```js
const h = x => 1 / (1 + Math.exp(-x));
```

#### 계단 함수 구현하기
```js
const stepFn = x => x > 0 ? 1 : 0;

const x = [-.5, 5, .1];
const y = x.map(stepFn);
console.log(y); // [0, 1, 1]
```
#### 시그모이드 함수 구현하기
```js
const sigmoid = x => 1 / (1 + Math.exp(-x));

const x = [-1, 1, 2];
const y = x.map(sigmoid);
console.log(y); // [0.2689414213699951, 0.7310585786300049, 0.8807970779778823]
```

### 비선형 함수
계단 함수와 시그모이드 함수의 공통점은 그 밖에도 있습니다. 중요한 공통점으로, 둘 모두는 비선형 함수입니다. 시그모이드 함수는 곡선, 계단 함수는 계단 처럼 구부러진 직선으로 나타나며, 동시에 비선형 함수로 분류됩니다.

선형 함수는 변환기에 무언가 입력했을 때 출력이 입력의 상수배만큼 변하는 함수입니다. 수식으로는 `f(x) = ax + b`이고, 이때 a와 b는 상수입니다. 그래서 선형 함수는 곧은 1개의 직선이 됩니다.

#### ReLU 함수
시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나, 최근에는 ReLU(Rectified Linear Unit)함수를 주로 이용합니다. ReLU는 입력이 0이 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수입니다.

```js
const ReLU1 = x => x > 0 ? x : 0;
const ReLU2 = x => Math.min(x, 0);
```

### 다차원 배열의 계산
다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있습니다.

#### 다차원 배열
다차원 배열도 그 기본은 `숫자의 집합`입니다. 숫자가 한 줄로 늘어선 것이나 직사각형으로 늘어놓은 것, 3차원으로 늘어놓은 것이나 N차원으로 나열하는 것을 통틀어 다차원 배열이라고 합니다.

```js
const ndim = list => Array.isArray(list[0]) ? list[0].length : 1;
const shape = list => Array.isArray(list[0]) ? [list.length, list[0].length] : [list.length, null];
```
```js
const vector = [1, 2, 3, 4];
const matrix = [[1, 2], [2, 3], [3, 4]];
```
